max_epochs: 2000
batch_size: 200
optimizer: AdamOptimizer
optimizer_cfg:
  learning_rate: 0.001
  beta1: 0.95
  beta2: 0.999
kl_annealing:
  start: 1
  interval: 300
  step: 0.1
  max: 1
dropout_keep_prob: 1
print_every: 1
print_tensor_names: [batch_rec_loss, batch_kl_loss, average_used_dims]

# log_input: true
# stop_threshold: 50
# load_best_and_test: true
stop_threshold: 2000
# lr_threshold: 0.00005
check_decay_threshold: 300
learning_rate_decay: 0.5
snapshot_every: 100
check_early_stop_every: 50
